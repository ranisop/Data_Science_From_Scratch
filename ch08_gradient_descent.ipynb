{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1. 경사 하강법에 숨은 의미\n",
    "- gradient(경사, 기울기)는 함수가 가장 빠르게 증가할 수 있는 방향을 나타낸다.\n",
    "- 따라서 함수의 최대값을 구하는 방법 중 하나는 임의의 시작점을 잡은 후, gradient를 계산하고,<br> gradient의 방향(즉 함수의 출력값이 가장 많이 증가하는 방향)으로 조금 이동하는 과정을 여러 번 반복하는 것이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_squares(v):\n",
    "    '''v에 속해 있는 항목들으리 제곱합을 계산한다.'''\n",
    "    return sum(v_i ** 2 for v_i in v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2. Gradient 계산하기\n",
    "- f가 단변수 함수인 경우, 점 x에서의 미분값은 x가 아주 조금 변했을 때 f(x)의 변화량을 의미한다.\n",
    "- x의 변화량을 다음 식에서는 h로 표현했으며, 아주 조금 변한다는 것을 반영하기 위해 h를 0에 점근하게 했다.\n",
    "- 이때 미분값은 함수 변화율(difference quotient)의 극한값이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference_quotient(f, x, h):\n",
    "    return (f(x + h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x * x\n",
    "\n",
    "def derivative(x):\n",
    "    return 2 * x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative_estimate = lambda x: differene_quotient(square, x, h=0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 두 계산식에 따른 결과값이 거의 비슷함을 보여 주기 위한 그래프\n",
    "import matplotlib.pyplot as plt    \n",
    "\n",
    "def plot_estimated_derivative():\n",
    "    \n",
    "    derivative_estimate = lambda x: difference_quotient(square, x, h=0.00001)\n",
    "    x = range(-10, 10)\n",
    "\n",
    "    plt.title('Actual Derivatives vs. Estimates')\n",
    "    plt.plot(x, list(map(derivative, x)), 'rx', label='Actual')  # 빨간색 x\n",
    "    plt.plot(x, list(map(derivative_estimate, x)), 'b+', label='Estimate')  # 파란색 +\n",
    "    plt.legend(loc=9)  # 상단 중앙 범례\n",
    "    plt.show()  # 보라색 *가 나와 주길!\n",
    "    \n",
    "plot_estimated_derivative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 만약 f가 다변수 함수라면 여러 개의 입력 변수 중 하나에 작은 변화가 있을 때 f(x)의 변화량을 알려주는 편도함수(partial derivative) 역시 여러 개 존재한다.\n",
    "- i번째 편도함수는, i번째 변수를 제외한 다른 모든 입력변수를 고정시켜서 계산할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_difference_quotient(f, v, i, h):\n",
    "    '''함수 f의 i번째 편도함수가 v에서 가지는 값'''\n",
    "    w = [v_j + (h if j == i else 0)  # h를 v의 i번째 변수에만 더해주자\n",
    "         for j, v_j in enumerate(v)]\n",
    "    \n",
    "    return(f(w) - f(v)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_gradient(f, v, h=0.00001):\n",
    "    return[partial_difference_quotient(f, v, i, h)\n",
    "          for i, _ in enumerate(v)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 함수 두 값의 차이를 사용해서 근사하는 것의 가장 큰 단점은 계산 비용이 크다는 것이다. <br> v의 길이가 n이면 estimate_gradient 함수는 f를 2n번 계산해야 한다. <br> 따라서 gradient를 빈번하게 계산하는 경우, 불필요한 계산도 많이 하게 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3. Gradient 적용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "minimum v [-0.00028293269772384306, 8.083791363538443e-05, -0.00040418956817692625]\n",
      "minimum value 2.4995488674526355e-07\n"
     ]
    }
   ],
   "source": [
    "from linear_algebra import distance\n",
    "\n",
    "def step(v, direction, step_size):\n",
    "    '''v에서 step_size만큼 이동하기'''\n",
    "    return [v_i + step_size * direction_i\n",
    "           for v_i, direction_i in zip(v, direction)]\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# 임의의 시작점을 선택\n",
    "v = [random.randint(-10, 10) for i in range(3)]\n",
    "\n",
    "tolerance = 0.0000001\n",
    "\n",
    "while True:\n",
    "    gradient = sum_of_squares_gradient(v)  # v의 경사도 계산\n",
    "    next_v = step(v, gradient, -0.0001)  # 경사도의 음수만큼 이동\n",
    "    if distance(next_v, v) < tolerance:  # tolerance만큼 수렴하게 되면 멈춤\n",
    "        break\n",
    "    v = next_v  # 아니면 계속 반복\n",
    "\n",
    "print(\"minimum v\", v)\n",
    "print(\"minimum value\", sum_of_squares(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4. 적절한 이동 거리 정하기\n",
    "- 이동 거리를 고정\n",
    "- 시간에 따라 이동 거리를 점차 줄임\n",
    "- 이동할 때마다 목적 함수를 최소화하는 이동 거리로 정함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 중 몇몇은 f에 부적합한 값을 넣어 오류를 발생시킬 수도 있다.\n",
    "# f에 오류가 발생했을 때 무한대(infinity)를 반환하기로 한다.\n",
    "def safe(f):\n",
    "    '''f와 똑같은 함수를 반환하지만 f에 오류가 발생하면 무한대를 반환해준다'''\n",
    "    def safe_f(*args, **kwargs):\n",
    "        try:\n",
    "            return f(*args, **kwargs)\n",
    "        except:\n",
    "            return float('inf')  # 파이썬에서는 무한대를 'inf'로 표기\n",
    "    return safe_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.5. 종합하기\n",
    "\n",
    "대부분의 경우 최소화하려는 함수 target_fn과 그에 대한 gradient 함수 gradient_fn이 존재한다. <br> 예를 들어, target_fn이 특정 모델의 파라미터(parameter)에 대한 오류값 함수라고 하자. 경사 하강법을 이용하면 오류값을 최소화하는 파라미터를 찾을 수 있다. <br> 더불어 파라미터 theta_0의 시작점을 (어떻게든) 구했다고 해봊. 그러면 경사 하강법을 다음과 같이 구현할 수 있다.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# minmize_batch -> 반복문을 돌 때마다 데이터셋 전체를 살펴봄\n",
    "\n",
    "def minmize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    '''목적 함수를 최소화시키는 theta를 경사 하강법을 사용해서 찾아준다.'''\n",
    "    step_sizes = [100, 10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "    \n",
    "    theta = theta_0  # theta를 시작점으로 설정\n",
    "    target_fn = safe(target_fn)  # 오류를 처리할 수 있는 target_fn으로 변환\n",
    "    value = target_fn(theta)  # 최소화시키려는 값\n",
    "    \n",
    "    while True:\n",
    "        gradient = gradient_fn(theta)\n",
    "        next_thetas = [step(theta, gradient, -step_size)\n",
    "                      for step_size in step_sizes]\n",
    "        \n",
    "        # 함수를 최소화시키는 theta 선택\n",
    "        next_theta = min(next_thetas, key=target_fn)\n",
    "        next_value = target_fn(next_theta)\n",
    "        \n",
    "        # tolerance만큼 수렴하면 멈춤\n",
    "        if abs(value - next_value) < tolerance:\n",
    "            return theta\n",
    "        else:\n",
    "            theta, value = next_theta, next_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def negate(f):\n",
    "    '''x를 입력하면 -f(x)를 반환해 주는 함수 생성'''\n",
    "    return lambda *args, **kwargs: -f(*args, **kwargs)\n",
    "\n",
    "def negate_all(f):\n",
    "    '''f가 여러 숫자를 반환할 때 모든 숫자를 음수로 변환'''\n",
    "    return lambda *args, **kwargs: [-y for y in f(*args, **kwargs)]\n",
    "\n",
    "def maximize_batch(target_fn, gradient_fn, theta_0, tolerance=0.000001):\n",
    "    return minmize_batch(negate(target_fn),\n",
    "                        negate_all(gradient_fn),\n",
    "                        theta_0,\n",
    "                        tolerance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.6. SGD(stochastic gradient descent)\n",
    "\n",
    "minmize_batch는 반복문을 돌 때마다 데이터 전체에 대해 gradient 값을 계산해야 하기 때문에 계산 시간이 오래 걸린다. <br> 그런데 대부분의 오류 함수는 더할 수 있는(additive) 속성을 가지고 있다. 즉, 데이터 전체에 대한 오류값이 각각 데이터 포인트에 대한 오류값의 합과 같다. <br> 이럴 때는 한 번 반복문을 돌 때마다 데이터 포인트 한 개에 대한 gradient를 계산하는 SGD를 사용할 수 있다. SGD는 수렴할 때까지 전체 데이터셋을 반복적으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def in_random_order(data):\n",
    "    '''임의의 순서로 data의 데이터 포인트를 반환'''\n",
    "    indexes = [i for i, _ in enumerate(data)]  # 데이터 포인트의 인덱스를 list로 생성\n",
    "    random.shuffle(indexes)  # 인덱스를 섞고\n",
    "    for i in indexes:  # 그 순서대로 데이터를 반환\n",
    "        yield data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    data = zip(x, y)\n",
    "    theta = theta_0  # 첫 시작점\n",
    "    alpha = alpha_0  # 기본 이동 거리\n",
    "    min_theta, mun_value = None, float('inf')  # 시작할 때의 최솟값\n",
    "    iterations_with_no_improvement = 0\n",
    "    \n",
    "    # 만약 100번 넘게 반복하는 동안 더 작아지지 않는다면 멈춤\n",
    "    while iterations_with_no_improvement < 100:\n",
    "        value = sum(target_fn(x_i, y_i, theta) for x_i, y_i in data)\n",
    "        \n",
    "        if value < min_value:\n",
    "            # 새로운 최솟값을 찾았다면 이 값을 저장하고\n",
    "            # 기본 이동 거리롤 다시 돌아감\n",
    "            min_theta, min_value = theta, value\n",
    "            iterations_with_no_improvement = 0\n",
    "            alpha = alpha_0\n",
    "        else:\n",
    "            # 만약 치ㅗ솟값이 줄어들지 않는다면 이동 거리를 축소\n",
    "            iterations_with_no_improvement += 1\n",
    "            alpha *= 0.9\n",
    "            \n",
    "        # 각 데이터 포인트에 대해 경사를 계산\n",
    "        for x_i, y_i in in_random_order(data):\n",
    "            gradient_i = gradient_fn(x_i, y_i, theta)\n",
    "            theta = vector_subtract(theta, scalar_multiply(alpha, gradient_i))\n",
    "            \n",
    "    return min_theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maximize_stochastic(target_fn, gradient_fn, x, y, theta_0, alpha_0=0.01):\n",
    "    return minimize_stochastic(negate(target_fn),\n",
    "                              negate_all(gradient_fn),\n",
    "                              x, y, theta_0, alpha_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
